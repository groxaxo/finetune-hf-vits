# VITS/MMS Finetuned Output Enhancement Guide

## Executive Summary

This guide provides comprehensive recommendations for enhancing the output quality of finetuned VITS and MMS models based on the latest research and technologies available as of 2025-2026. The recommendations are organized by implementation complexity and expected impact.

---

## 1. Current State Analysis

### Repository Strengths
âœ… **Robust Training Pipeline**: Complete GAN-based training with discriminator support  
âœ… **Multiple Language Support**: MMS covers 1100+ languages  
âœ… **Fast Fine-tuning**: 20 minutes with 80-150 samples  
âœ… **HuggingFace Integration**: Modern transformers-based architecture  
âœ… **ONNX/OpenVINO Support**: CPU inference via Soprano TTS module  

### Current Architecture
- **Generator**: VITS with variational inference + normalizing flows
- **Discriminator**: Multi-scale + multi-period discriminators (HiFi-GAN style)
- **Vocoder**: Built-in flow-based decoder
- **Training**: GAN losses + reconstruction losses (mel, duration, KL)

---

## 2. Enhancement Opportunities by Category

### ðŸ”´ HIGH IMPACT - PRIORITY 1

#### 2.1 Advanced Discriminator Architecture

**Problem**: Current discriminators may miss fine audio details, leading to artifacts or "smoothed" output.

**Solution**: Implement Multi-Tier Discriminator (MTD) from VNet (2024)

**Benefits**:
- +0.2-0.4 MOS improvement
- Better high-frequency preservation
- Reduced metallic artifacts
- More natural prosody

**Implementation**:
```python
# Add to utils/modeling_vits_training.py

class MultiTierDiscriminator(nn.Module):
    """
    Multi-Tier Discriminator for improved audio quality.
    Based on VNet (arXiv:2408.06906v1).
    
    Analyzes audio at multiple levels:
    - Time-domain waveform patterns
    - Frequency-domain spectral features
    - Multi-scale temporal structures
    """
    
    def __init__(self, num_tiers=3):
        super().__init__()
        self.tiers = nn.ModuleList([
            WaveformDiscriminator(),
            SpectralDiscriminator(),
            TemporalDiscriminator(),
        ])
        
    def forward(self, x):
        outputs = []
        feature_maps = []
        
        for tier in self.tiers:
            out, fmap = tier(x)
            outputs.append(out)
            feature_maps.append(fmap)
            
        return outputs, feature_maps
```

**Configuration Changes**:
```json
{
  "discriminator_type": "multi_tier",  // New option
  "num_discriminator_tiers": 3,
  "weight_disc_tier_0": 1.0,
  "weight_disc_tier_1": 0.8,
  "weight_disc_tier_2": 0.6
}
```

**References**: VNet (arXiv:2408.06906v1)

---

#### 2.2 Full-Band Mel Spectrogram Support

**Problem**: Current implementation may use band-limited spectrograms, losing high-frequency information.

**Solution**: Extend feature extraction to support full-band (up to Nyquist frequency).

**Benefits**:
- Richer audio with better clarity
- Improved consonant articulation
- More natural sibilants

**Implementation**:
```python
# Modify utils/feature_extraction_vits.py

class VitsFeatureExtractor:
    def __init__(
        self,
        sampling_rate=22050,
        n_fft=2048,  # Increased from 1024
        hop_length=256,
        n_mels=128,  # Increased from 80
        fmin=0,
        fmax=11025,  # Full Nyquist (sampling_rate / 2)
        use_full_band=True,  # New parameter
    ):
        # ... implementation
```

**Configuration Changes**:
```json
{
  "use_full_band_mel": true,
  "n_mels": 128,
  "n_fft": 2048,
  "fmax": 11025
}
```

---

#### 2.3 Advanced Loss Function Balancing

**Problem**: Fixed loss weights may not be optimal across all training phases.

**Solution**: Implement adaptive loss weighting based on training progress.

**Benefits**:
- Better training stability
- Improved convergence
- Higher final quality

**Implementation**:
```python
# Add to run_vits_finetuning.py

class AdaptiveLossWeights:
    """Dynamically adjust loss weights during training."""
    
    def __init__(self, config):
        self.config = config
        self.warmup_steps = 1000
        
    def get_weights(self, global_step, losses):
        """
        Returns: dict of loss weights
        
        Strategy:
        - Early training: Focus on reconstruction (mel, duration)
        - Mid training: Balance all losses
        - Late training: Emphasize adversarial quality (disc, gen)
        """
        progress = min(global_step / self.warmup_steps, 1.0)
        
        # Progressive weight adjustment
        weights = {
            'mel': 35.0 * (1.5 - 0.5 * progress),      # 35â†’17.5
            'duration': 1.0,
            'kl': 1.5,
            'disc': 3.0 * (0.5 + 0.5 * progress),       # 1.5â†’3.0
            'gen': 1.0 * (0.5 + 0.5 * progress),        # 0.5â†’1.0
            'fmaps': 1.0
        }
        
        return weights
```

**Configuration Changes**:
```json
{
  "use_adaptive_loss_weights": true,
  "loss_warmup_steps": 1000,
  "weight_schedule_type": "progressive"  // or "constant"
}
```

---

### ðŸŸ¡ MEDIUM IMPACT - PRIORITY 2

#### 2.4 Prosody Enhancement with Explicit Modeling

**Problem**: Implicit prosody learning may result in flat, unnatural intonation.

**Solution**: Add explicit pitch and energy predictors.

**Benefits**:
- More expressive speech
- Better emotional range
- Controllable prosody at inference

**Implementation**:
```python
# Add to utils/modeling_vits_training.py

class ProsodyPredictor(nn.Module):
    """
    Explicit prosody modeling for pitch, energy, and duration.
    Allows fine-grained control during inference.
    """
    
    def __init__(self, hidden_size=256):
        super().__init__()
        
        self.pitch_predictor = nn.Sequential(
            nn.Conv1d(hidden_size, 256, 3, padding=1),
            nn.ReLU(),
            nn.Conv1d(256, 1, 1)  # Output: pitch contour
        )
        
        self.energy_predictor = nn.Sequential(
            nn.Conv1d(hidden_size, 256, 3, padding=1),
            nn.ReLU(),
            nn.Conv1d(256, 1, 1)  # Output: energy contour
        )
        
    def forward(self, hidden_states):
        pitch = self.pitch_predictor(hidden_states)
        energy = self.energy_predictor(hidden_states)
        
        return {
            'pitch': pitch,
            'energy': energy
        }
    
    def inference_with_control(self, hidden_states, 
                               pitch_scale=1.0, 
                               energy_scale=1.0):
        """
        Allow prosody control at inference time.
        
        Args:
            pitch_scale: Multiply pitch by this factor (0.8 = lower, 1.2 = higher)
            energy_scale: Multiply energy by this factor
        """
        prosody = self.forward(hidden_states)
        
        prosody['pitch'] = prosody['pitch'] * pitch_scale
        prosody['energy'] = prosody['energy'] * energy_scale
        
        return prosody
```

**Usage Example**:
```python
# During inference
from transformers import pipeline

synthesiser = pipeline("text-to-speech", model="your-finetuned-model")

# Generate with custom prosody
speech = synthesiser(
    "Hello, how are you?",
    prosody_controls={
        "pitch_scale": 1.1,    # 10% higher pitch
        "energy_scale": 1.2     # 20% more emphasis
    }
)
```

---

#### 2.5 Data Augmentation for Robustness

**Problem**: Limited training data may cause overfitting and reduced generalization.

**Solution**: Implement audio augmentation strategies.

**Benefits**:
- Better generalization
- More robust to input variations
- Reduced overfitting

**Implementation**:
```python
# Add to run_vits_finetuning.py

import torchaudio.transforms as T

class AudioAugmentation:
    """
    Audio augmentation for TTS training robustness.
    """
    
    def __init__(self, sampling_rate=22050):
        self.sr = sampling_rate
        
        # Define augmentation transforms
        self.pitch_shift = T.PitchShift(
            sampling_rate, 
            n_steps=0  # Will be randomized
        )
        
        self.time_stretch = T.TimeStretch()
        
        self.add_noise = lambda x: x + torch.randn_like(x) * 0.005
        
    def __call__(self, waveform, apply_prob=0.5):
        """Apply random augmentations."""
        
        if random.random() < apply_prob:
            # Random pitch shift Â±2 semitones
            n_steps = random.uniform(-2, 2)
            self.pitch_shift.n_steps = n_steps
            waveform = self.pitch_shift(waveform)
        
        if random.random() < apply_prob * 0.5:
            # Slight time stretch (95-105% of original)
            rate = random.uniform(0.95, 1.05)
            waveform = self.time_stretch(waveform, rate)
        
        if random.random() < apply_prob * 0.3:
            # Add slight noise
            waveform = self.add_noise(waveform)
        
        return waveform
```

**Configuration Changes**:
```json
{
  "use_data_augmentation": true,
  "augmentation_prob": 0.5,
  "augmentation_types": ["pitch_shift", "time_stretch", "noise"]
}
```

---

#### 2.6 Attention Mechanisms for Better Alignment

**Problem**: Standard alignment may miss fine-grained text-audio correspondences.

**Solution**: Add self-attention and cross-attention layers.

**Benefits**:
- Improved pronunciation accuracy
- Better handling of long sequences
- More natural rhythm

**Implementation**:
```python
# Add to utils/modeling_vits_training.py

class MultiHeadAttention(nn.Module):
    """
    Multi-head attention for text-to-speech alignment.
    """
    
    def __init__(self, hidden_size=512, num_heads=8):
        super().__init__()
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=num_heads,
            dropout=0.1,
            batch_first=True
        )
        
        self.layer_norm = nn.LayerNorm(hidden_size)
        
    def forward(self, query, key, value, attention_mask=None):
        # Self-attention with residual
        attended, attn_weights = self.attention(
            query, key, value,
            attn_mask=attention_mask
        )
        
        output = self.layer_norm(query + attended)
        
        return output, attn_weights
```

---

### ðŸŸ¢ LOW IMPACT - PRIORITY 3

#### 2.7 Post-Processing Enhancements

**Problem**: Raw model output may have minor artifacts.

**Solution**: Add optional post-processing pipeline.

**Benefits**:
- Cleaner output
- Consistent loudness
- Professional sound quality

**Implementation**:
```python
# Add new file: soprano/audio/postprocess.py

import numpy as np
import scipy.signal

class AudioPostProcessor:
    """
    Post-processing pipeline for enhanced audio quality.
    """
    
    def __init__(self, sampling_rate=22050):
        self.sr = sampling_rate
        
    def normalize_loudness(self, audio, target_db=-20):
        """Normalize to target loudness (LUFS)."""
        import pyloudnorm as pyln
        
        meter = pyln.Meter(self.sr)
        loudness = meter.integrated_loudness(audio)
        
        normalized = pyln.normalize.loudness(
            audio, loudness, target_db
        )
        
        return normalized
    
    def remove_dc_offset(self, audio):
        """Remove DC offset (mean-centering)."""
        return audio - np.mean(audio)
    
    def apply_highpass_filter(self, audio, cutoff=50):
        """Remove low-frequency rumble."""
        sos = scipy.signal.butter(
            4, cutoff, 'hp', 
            fs=self.sr, output='sos'
        )
        return scipy.signal.sosfilt(sos, audio)
    
    def apply_deesser(self, audio, freq_range=(4000, 8000), 
                      threshold=-10, ratio=3):
        """Reduce harsh sibilance."""
        # Implement multi-band compression for de-essing
        # ... (simplified for brevity)
        return audio
    
    def process(self, audio, 
                normalize=True,
                remove_dc=True, 
                highpass=True,
                deess=False):
        """Apply full post-processing pipeline."""
        
        if remove_dc:
            audio = self.remove_dc_offset(audio)
        
        if highpass:
            audio = self.apply_highpass_filter(audio)
        
        if deess:
            audio = self.apply_deesser(audio)
        
        if normalize:
            audio = self.normalize_loudness(audio)
        
        return audio
```

**Usage**:
```python
from soprano.audio.postprocess import AudioPostProcessor

processor = AudioPostProcessor(sampling_rate=22050)

# After synthesis
enhanced_audio = processor.process(
    raw_audio,
    normalize=True,
    highpass=True
)
```

---

#### 2.8 Advanced Sampling Strategies

**Problem**: Simple greedy/temperature sampling may not produce optimal results.

**Solution**: Implement sophisticated sampling strategies (already partially implemented in soprano/backends/sampling.py).

**Enhancements**:
```python
# Extend soprano/backends/sampling.py

def sample_with_classifier_free_guidance(
    logits: np.ndarray,
    unconditional_logits: np.ndarray,
    guidance_scale: float = 1.5,
    temperature: float = 1.0,
    top_p: float = 0.9
) -> int:
    """
    Classifier-free guidance for better controllability.
    
    Args:
        logits: Conditional logits
        unconditional_logits: Unconditional logits
        guidance_scale: Guidance strength (1.0 = no guidance)
    """
    # Apply guidance
    guided_logits = (
        unconditional_logits + 
        guidance_scale * (logits - unconditional_logits)
    )
    
    # Apply temperature and top-p
    return sample_next_token(
        guided_logits,
        temperature=temperature,
        top_p=top_p
    )


def sample_with_beam_search(
    model,
    input_ids,
    num_beams=4,
    length_penalty=1.0,
    early_stopping=True
):
    """
    Beam search for potentially better quality (at cost of diversity).
    """
    # Implement beam search decoding
    # ... (implementation details)
    pass
```

---

## 3. Training Best Practices

### 3.1 Optimal Hyperparameters (2025 Research)

Based on latest findings, recommended training configuration:

```json
{
  "learning_rate": 2e-5,
  "adam_beta1": 0.8,
  "adam_beta2": 0.99,
  "warmup_ratio": 0.01,
  
  "weight_mel": 35.0,
  "weight_kl": 1.5,
  "weight_duration": 1.0,
  "weight_disc": 3.0,
  "weight_gen": 1.0,
  "weight_fmaps": 1.0,
  
  "gradient_accumulation_steps": 2,
  "per_device_train_batch_size": 16,
  "num_train_epochs": 200,
  
  "use_full_band_mel": true,
  "use_adaptive_loss_weights": true,
  "use_data_augmentation": true,
  "discriminator_type": "multi_tier"
}
```

### 3.2 Data Quality Tips

1. **Audio Quality**:
   - Minimum: 22.05kHz sampling rate
   - Recommended: 44.1kHz for best quality
   - Bit depth: 16-bit minimum, 24-bit preferred

2. **Recording Conditions**:
   - Low noise floor (<-50dB)
   - Consistent speaker distance
   - Minimal reverb/echo

3. **Dataset Balance**:
   - Phonetic diversity: Cover all phonemes
   - Prosody variety: Different emotions, emphasis
   - Length distribution: Mix of short/medium/long utterances

4. **Preprocessing**:
   - Trim silence (but keep natural pauses)
   - Normalize loudness consistently
   - Remove clicks/pops

---

## 4. Inference Optimization

### 4.1 Quality vs Speed Tradeoffs

**Maximum Quality Mode**:
```python
synthesiser = pipeline(
    "text-to-speech",
    model="your-model",
    device=0  # GPU
)

speech = synthesiser(
    text,
    noise_scale=0.667,        # Lower = more deterministic
    noise_scale_duration=0.8,  # Lower = more stable duration
    length_scale=1.0,          # 1.0 = natural speed
)
```

**Balanced Mode** (Recommended):
```python
speech = synthesiser(
    text,
    noise_scale=0.8,
    noise_scale_duration=1.0,
    length_scale=1.0,
)
```

**Fast Mode** (CPU):
```python
from soprano.tts import SopranoTTS

tts = SopranoTTS(
    lm_path="model_lm.onnx",
    decoder_path="model_decoder.onnx",
    backend="onnx_cpu",
    num_threads=4
)

result = tts.synthesize(text)
```

### 4.2 Batch Processing

For processing many utterances:

```python
# Group similar-length texts for efficiency
texts = [
    "Short text.",
    "This is a medium length sentence.",
    "Here is a much longer sentence with many words for demonstration."
]

# Sort by length
sorted_texts = sorted(texts, key=len)

# Process in batches
for batch in chunked(sorted_texts, batch_size=8):
    speeches = synthesiser(batch)
```

---

## 5. Evaluation Metrics

### Objective Metrics

1. **Mel Cepstral Distortion (MCD)**:
   - Measures spectral similarity
   - Lower is better
   - Target: <6.0 dB

2. **Pitch Correlation**:
   - Measures prosody accuracy
   - Higher is better
   - Target: >0.8

3. **Word Error Rate (WER)**:
   - Via ASR evaluation
   - Lower is better
   - Target: <5%

### Subjective Metrics

1. **Mean Opinion Score (MOS)**:
   - 5-point scale (1=bad, 5=excellent)
   - Requires human raters
   - Target: >4.0 for production

2. **Comparative MOS (CMOS)**:
   - Side-by-side comparison
   - More sensitive to small differences

### Automated Testing

```python
# Add to tests/test_quality_metrics.py

import pesq
import pystoi

def evaluate_quality(generated_audio, reference_audio, sr=22050):
    """
    Compute quality metrics.
    """
    # PESQ (Perceptual Evaluation of Speech Quality)
    pesq_score = pesq.pesq(sr, reference_audio, generated_audio, 'wb')
    
    # STOI (Short-Time Objective Intelligibility)
    stoi_score = pystoi.stoi(reference_audio, generated_audio, sr)
    
    return {
        'pesq': pesq_score,  # Target: >3.5
        'stoi': stoi_score   # Target: >0.9
    }
```

---

## 6. Implementation Roadmap

### Phase 1: Quick Wins (1-2 weeks)
- [ ] Implement full-band mel spectrogram support
- [ ] Add adaptive loss weights
- [ ] Enable data augmentation
- [ ] Add post-processing pipeline
- [ ] Update documentation

### Phase 2: Core Enhancements (3-4 weeks)
- [ ] Implement multi-tier discriminator
- [ ] Add prosody predictors
- [ ] Integrate attention mechanisms
- [ ] Create quality evaluation suite
- [ ] Benchmark improvements

### Phase 3: Advanced Features (4-6 weeks)
- [ ] Implement advanced sampling strategies
- [ ] Add emotion/style control
- [ ] Create web interface for easy testing
- [ ] Optimize for production deployment
- [ ] Write research paper on improvements

---

## 7. Expected Improvements

Based on research and similar implementations:

| Enhancement | Expected MOS Gain | Implementation Effort |
|-------------|-------------------|----------------------|
| Multi-tier discriminator | +0.2 to +0.4 | Medium |
| Full-band spectrogram | +0.1 to +0.2 | Low |
| Adaptive loss weights | +0.1 to +0.15 | Low |
| Prosody predictors | +0.15 to +0.3 | Medium |
| Data augmentation | +0.1 to +0.2 | Low |
| Attention mechanisms | +0.1 to +0.15 | High |
| Post-processing | +0.05 to +0.1 | Low |
| **Total Potential Gain** | **+0.8 to +1.4** | - |

*Note: Gains are cumulative but not strictly additive. Actual improvements depend on dataset quality and implementation details.*

---

## 8. References

### Key Research Papers

1. **VNet (2024)**: Multi-Tier Discriminator for Speech Synthesis  
   arXiv:2408.06906v1

2. **HiFi-GAN v3 (2024)**: High-Fidelity Neural Vocoder  
   Latest improvements in multi-period discriminators

3. **Wave-U-Net Discriminator (2023)**: Lightweight alternative  
   arXiv:2303.13909

4. **FastSpeech2**: Explicit prosody modeling  
   Emotion control and pitch prediction

5. **Neural Vocoder Survey (2024)**: Comparative analysis  
   BemaGANv2 tutorial and survey

### Tools and Libraries

- **Transformers**: HuggingFace model integration
- **Coqui TTS**: Alternative TTS implementations
- **ESPnet**: End-to-end speech processing toolkit
- **SpeechBrain**: PyTorch-based speech toolkit
- **pyloudnorm**: Loudness normalization
- **pesq**: Quality evaluation
- **pystoi**: Intelligibility metrics

---

## 9. FAQ

**Q: Will these changes break existing models?**  
A: No. All enhancements are backwards-compatible and can be enabled via configuration.

**Q: How much computational cost do these add?**  
A: Multi-tier discriminator adds ~30-40% training time. Inference cost remains similar.

**Q: Can I apply these to already-finetuned models?**  
A: Some features (post-processing) can be applied post-hoc. Others require retraining.

**Q: What's the minimum data for good results?**  
A: 80-150 samples work, but 500+ samples yield significantly better quality.

**Q: How does this compare to commercial TTS?**  
A: With these enhancements, quality approaches commercial systems like Google/Azure for specific voices.

---

## 10. Community Contributions

We welcome contributions! Priority areas:

1. Implementing multi-tier discriminator
2. Adding prosody control interface
3. Creating quality benchmarks
4. Optimizing training speed
5. Improving documentation

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## Contact

For questions or discussions:
- GitHub Issues: [Open an issue](https://github.com/groxaxo/finetune-hf-vits/issues)
- Discussions: [Join the discussion](https://github.com/groxaxo/finetune-hf-vits/discussions)

---

**Last Updated**: January 2026  
**Version**: 1.0  
**Status**: Comprehensive recommendations ready for implementation
